{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3zWOAKCU8u1"
      },
      "source": [
        "### Namaste! This notebook demonstrates question answering on Hindi data using Indic LLM - Airavata, a multilingual embedding model and Chroma db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbNjiUDMVRDL"
      },
      "source": [
        "Lets get started by collecting dataset, if you already have the dataset, parsed and prepared, you can skip through this part. We will be taking 5 URLs related to income tax, the url comprises of faq as well as unstructured text. The topics discussed include various sections for deduction on tax, various faqs related to ITR1 for indiviudals and various forms required."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crawling URLs"
      ],
      "metadata": {
        "id": "7IuMjLOmLWm1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "CB5WNidJVQCS"
      },
      "outputs": [],
      "source": [
        "urls =['https://www.incometax.gov.in/iec/foportal/hi/help/e-filing-itr1-form-sahaj-faq',\n",
        "        'https://www.incometax.gov.in/iec/foportal/hi/help/e-filing-itr4-form-sugam-faq',\n",
        "       'https://navbharattimes.indiatimes.com/business/budget/budget-classroom/income-tax-sections-know-which-section-can-save-how-much-tax-here-is-all-about-income-tax-law-to-understand-budget-speech/articleshow/89141099.cms',\n",
        "       'https://www.incometax.gov.in/iec/foportal/hi/help/individual/return-applicable-1',\n",
        "       'https://www.zeebiz.com/hindi/personal-finance/income-tax/tax-deductions-under-section-80g-income-tax-exemption-limit-how-to-save-tax-on-donation-money-to-charitable-trusts-126529'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-itrJJ_WCrR"
      },
      "source": [
        "\n",
        "I will be using one of my favorite libraries to crawl website - [Markdown Crawler](https://github.com/paulpierre/markdown-crawler). You can install it using the command mentioned below. It parses the website into markdown format and stores them in markdown files. ANd one interesting thing, although we will be crawling only the urls, but it also has the capability to parse linked urls to website(look at the depth paramaneter).\n",
        "For now lets continue with what we were doing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2I-dRO6Wpj6",
        "outputId": "0134b221-c744-4275-aea6-8b01bd70d1f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: markdown-crawler in /usr/local/lib/python3.10/dist-packages (0.0.8)\n",
            "Requirement already satisfied: markdownify in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /usr/local/lib/python3.10/dist-packages (from markdownify) (4.12.3)\n",
            "Requirement already satisfied: six<2,>=1.15 in /usr/local/lib/python3.10/dist-packages (from markdownify) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5,>=4.9->markdownify) (2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install markdown-crawler\n",
        "!pip install markdownify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "3mivQpqLWxOo"
      },
      "outputs": [],
      "source": [
        "from markdown_crawler import md_crawl\n",
        "def crawl_urls(urls: list, storage_folder_path: str, max_depth=0):\n",
        "    \"\"\"Crawl a list of URLs and store results.\n",
        "    Parameters:\n",
        "    - urls: URLs to crawl.\n",
        "    - storage_folder_path: Location for results; folder is auto-created.\n",
        "    - max_depth: Link depth to crawl; 0 (recommended) means only the listed URLs.\n",
        "\n",
        "    \"\"\"\n",
        "    for url in urls:\n",
        "        print(f\"Crawling {url}\")\n",
        "        md_crawl(\n",
        "            url,\n",
        "            max_depth=max_depth,\n",
        "            base_dir=storage_folder_path,\n",
        "            is_links=True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iREe3hyVXPeC",
        "outputId": "dc49791f-7397-4e4b-b6f5-886c685e1705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling https://www.incometax.gov.in/iec/foportal/hi/help/e-filing-itr1-form-sahaj-faq\n",
            "Crawling https://www.incometax.gov.in/iec/foportal/hi/help/e-filing-itr4-form-sugam-faq\n",
            "Crawling https://navbharattimes.indiatimes.com/business/budget/budget-classroom/income-tax-sections-know-which-section-can-save-how-much-tax-here-is-all-about-income-tax-law-to-understand-budget-speech/articleshow/89141099.cms\n",
            "Crawling https://www.incometax.gov.in/iec/foportal/hi/help/individual/return-applicable-1\n",
            "Crawling https://www.zeebiz.com/hindi/personal-finance/income-tax/tax-deductions-under-section-80g-income-tax-exemption-limit-how-to-save-tax-on-donation-money-to-charitable-trusts-126529\n"
          ]
        }
      ],
      "source": [
        "crawl_urls(urls= urls, storage_folder_path = './incometax_documents/')\n",
        "#you do not need to make a folder intitially. Md Crawler handles that for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGEcMoo-Y60B"
      },
      "source": [
        "Once the urls have been crawled and data has been stored in markdown files, its time to parse the content in those files. But before that some notes on parsing:\n",
        "\n",
        "\n",
        "It is important to parse, because there is still some noise left in the markdown files, and the embedding model that we are gonna use and llm has token limit, so we have to not exceed that.\n",
        "\n",
        "Before we move on to the code, some parameters that I need to introduce. The embedding model that we are gonna use has a token limit of 512 tokens. It truncates after that. We will look more about this embedding model and why I chose this in the coming section but for now only thing we have to know is that the limit is 512 tokens so we will try to keep section less than 512 tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing and Chunking Documents"
      ],
      "metadata": {
        "id": "LSNzaasfLcj2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLlVCcqKal2c"
      },
      "source": [
        "Lets first write a function to extract content out of a file. We will be use python library markdown and beautifulsoup for it. Below are commands to install them"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsing"
      ],
      "metadata": {
        "id": "WZ47lHJgLii9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gmCtN9Kawjw",
        "outputId": "caaeb710-8936-40b6-aae0-2e2e6ed0620d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (3.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "kXjToC85aPZw"
      },
      "outputs": [],
      "source": [
        "# lets first write a function to extract content out of a file\n",
        "import markdown\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def read_markdown_file(file_path):\n",
        "    \"\"\"Read a Markdown file and extract its sections as headers and content.\"\"\"\n",
        "    # Open the markdown file and read its content\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        md_content = file.read()\n",
        "\n",
        "    # Convert markdown to HTML\n",
        "    html_content = markdown.markdown(md_content)\n",
        "\n",
        "    # Parse HTML content\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    sections = []\n",
        "    current_section = None\n",
        "\n",
        "    # Loop through HTML tags\n",
        "    for tag in soup:\n",
        "        # Start a new section if a header tag is found\n",
        "        if tag.name and tag.name.startswith('h'):\n",
        "            if current_section:\n",
        "                sections.append(current_section)\n",
        "            current_section = {'header': tag.text, 'content': ''}\n",
        "\n",
        "        # Add content to the current section\n",
        "        elif current_section:\n",
        "            current_section['content'] += tag.get_text() + '\\n'\n",
        "\n",
        "    # Add the last section\n",
        "    if current_section:\n",
        "        sections.append(current_section)\n",
        "\n",
        "    return sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "TqEtj4bga3I_"
      },
      "outputs": [],
      "source": [
        "#lets look at the output of one of the files:\n",
        "\n",
        "sections = read_markdown_file('./incometax_documents/business-budget-budget-classroom-income-tax-sections-know-which-section-can-save-how-much-tax-here-is-all-about-income-tax-law-to-understand-budget-speech-articleshow-89141099-cms.md')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDmhjpawbaKD"
      },
      "source": [
        "Oh yes! the content looks cleaner now, but the problem is sometimes the section are good,and sometiems they are note, some sections with especially with empty headers are unnecessary. So lets write a funcito to parse a section, we will pass  a particular section only if the header and content both are non empty and header does not belong to any of these - ['main navigation','navigation', 'footer']."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "oga2SeF_cVsL"
      },
      "outputs": [],
      "source": [
        "def pass_section(section):\n",
        "    # List of headers to ignore\n",
        "    headers_to_ignore = ['main navigation', 'navigation', 'footer', 'advertisement']\n",
        "\n",
        "    # Check if the header is not in the ignore list and both header and content are non-empty\n",
        "    if section['header'].lower() not in headers_to_ignore and section['header'].strip() and section['content'].strip():\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "f5wOvbVlnSMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9895178-92bb-4d03-fc31-1e47297ed7d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n"
          ]
        }
      ],
      "source": [
        "passed_sections = []\n",
        "import os\n",
        "# Iterate through all Markdown files in the folder\n",
        "for filename in os.listdir('incometax_documents'):\n",
        "    if filename.endswith('.md'):\n",
        "        file_path = os.path.join('incometax_documents', filename)\n",
        "        # Extract sections from the current Markdown file\n",
        "        all_sections = read_markdown_file(file_path)\n",
        "        # Filter sections based on the pass_section function\n",
        "        passed_sections.extend(section for section in all_sections if pass_section(section))\n",
        "\n",
        "print(len(passed_sections))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking"
      ],
      "metadata": {
        "id": "GNddkd7dLqBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding Model** <br>\n",
        "The embedding model that we are gonna use is [multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base).  As mentioned on its huggingface page it supports 100 languages, although low-resource languages may see performance degradation But i have observed it performs fairly decent for Hindi. If you want better accuracy you can use try [BGE M3](https://huggingface.co/BAAI/bge-m3) as well but that is pretty resource intesive. Also OpenAI embeddings might perform well here, but lets stick to everything opensource for now. Hence a light weight but decent model. E5 it is."
      ],
      "metadata": {
        "id": "Aibe5IDwKkOf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7y3Hezbcxv5"
      },
      "source": [
        "Before diving into the function, let's discuss chunking. Every Retrieval-Augmented Generation (RAG) system uses two key models: an 'Embedding Model' for generating embeddings (used for retrieval) and a 'Language Model' for generating answers. Both models have token limits, so it's essential to split unstructured content accordingly.\n",
        "\n",
        "<h3>Embedding Model Token Limit</h3>\n",
        "The embedding model we’re using, multilingual-e5-base, has a token limit of 512. While the language model we’ll use has a larger token limit, we’ll focus on the embedding model for now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Chunking Function</h3>\n",
        "<p>The <code>chunk_text</code> function splits a section of text into smaller, manageable chunks while ensuring that no chunk exceeds the specified token limit (512 tokens by default). It splits the text by single newlines to create logical subsections, then further divides them into chunks, maintaining overlap to preserve context between chunks. This approach ensures that even large sections of text are processed correctly without losing important information.</p>"
      ],
      "metadata": {
        "id": "MzkaPuFGJYaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(section, max_tokens=512, overlap=50):\n",
        "    header = section['header']\n",
        "    text = section['content']\n",
        "\n",
        "    # Split the content into smaller parts based on single newlines or other logical separators\n",
        "    sections = text.split(\"\\n\\n\")  # Split by single newlines first\n",
        "\n",
        "    all_chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    # Process each line (or smaller logical part) within the section\n",
        "    for line in sections:\n",
        "        # Tokenize by splitting by spaces to count tokens (words)\n",
        "        tokens = line.split()  # This treats each word as a token\n",
        "\n",
        "        # If adding this line to the current chunk doesn't exceed the max limit\n",
        "        if len(current_chunk.split()) + len(tokens) <= max_tokens:\n",
        "            # Add the line to the current chunk\n",
        "            current_chunk += \" \" + line\n",
        "        else:\n",
        "            # If the chunk exceeds the limit, create a new chunk with overlap\n",
        "            if len(current_chunk.split()) > max_tokens - overlap:\n",
        "                # Add the chunk with the overlap\n",
        "                all_chunks.append({'header': header, 'content': current_chunk.strip()})\n",
        "                current_chunk = \" \".join(tokens[-overlap:])  # Start new chunk with overlap\n",
        "            else:\n",
        "                # Add the current chunk and reset it for the next line\n",
        "                all_chunks.append({'header': header, 'content': current_chunk.strip()})\n",
        "                current_chunk = line  # Reset chunk to the current line\n",
        "\n",
        "    # Add the last chunk if it exists\n",
        "    if current_chunk.strip():\n",
        "        all_chunks.append({'header': header, 'content': current_chunk.strip()})\n",
        "\n",
        "    return all_chunks\n"
      ],
      "metadata": {
        "id": "URqSGkkpEHYS"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "passed_sections = [chunk for section in passed_sections for chunk in chunk_text(section)]"
      ],
      "metadata": {
        "id": "IOJrdqYjFiXu"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(passed_sections)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kumk4U67HHDy",
        "outputId": "c665991b-5e56-4e24-e8dd-ee55ee7cf1cb"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Vector Store and Ingesting documents"
      ],
      "metadata": {
        "id": "OTeCf9AbLOiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose Chroma DB as I could use it in Google Collab without any hosting and it's good for experimentation. But you could also use vector stores of your choice. Here's how you install it."
      ],
      "metadata": {
        "id": "_n8T4zFyIaiL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "OW2pTUF1iotl"
      },
      "outputs": [],
      "source": [
        "# !pip install chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This way to initiate Chroma DB creates an in-memory instance of Chroma. This is useful for testing and development, but not recommended for production use. For production you should host it, Please refer to its [documentation](https://docs.trychroma.com/deployment) for details."
      ],
      "metadata": {
        "id": "G_HAloobIoZ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "p2qTAf7bkOtj"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "chroma_client = chromadb.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chroma DB offers built-in support for open-source sentence transformers. Here's how to use it:"
      ],
      "metadata": {
        "id": "X4djoZ2OIx7F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "HuTEeLF4kWZj"
      },
      "outputs": [],
      "source": [
        "from chromadb.utils import embedding_functions\n",
        "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"intfloat/multilingual-e5-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create the collection! <br>\n",
        "**Note -** We use `metadata={\"hnsw:space\": \"cosine\"}` because ChromaDB's default distance is Euclidean, but cosine distance is typically preferred for RAG purposes."
      ],
      "metadata": {
        "id": "NwyWf1r6I3DO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "HPLqU0z8mLDu"
      },
      "outputs": [],
      "source": [
        "collection = chroma_client.create_collection(name=\"income_tax_hindi\", embedding_function= sentence_transformer_ef, metadata={\"hnsw:space\": \"cosine\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "hi0lAOs7mkgP"
      },
      "outputs": [],
      "source": [
        "#command for deletion in case you need to recreate it\n",
        "# chroma_client.delete_collection(name=\"income_tax_hindi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ingesting the documents"
      ],
      "metadata": {
        "id": "ClMzwG-BJNve"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "VkBfhPONmqVV"
      },
      "outputs": [],
      "source": [
        "collection.add(\n",
        "    documents=[section['content'] for section in passed_sections],\n",
        "    metadatas = [{'header': section['header']} for section in passed_sections],\n",
        "    ids=[str(i) for i in range(len(passed_sections))]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "UfUj4gzRn4Hx"
      },
      "outputs": [],
      "source": [
        "#querying the results\n",
        "docs = collection.query(\n",
        "    query_texts=[\"सेक्शन 80 C की लिमिट क्या होती है\"],\n",
        "    n_results=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0u8qqNUnx9I",
        "outputId": "989d7272-539f-4681-8602-5d0dfb7eb5bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': [['31', '33', '27']],\n",
              " 'embeddings': None,\n",
              " 'documents': [['सेक्शन 80डी के अलावा इनकम टैक्स कानून में दो और सेक्शन हैं, जिनका आप स्वास्थ्य से जुड़े खर्च का लाभ उठा सकते हैं। सेक्शन 80डीडी आप पर आश्रित किसी विकलांग व्यक्ति के लिए चिकित्सा खर्च से संबंधित है। आश्रित में जीवनसाथी, बच्चे, पैरेंट्स, भाई या बहन हो सकते हैं। इनकम टैक्स में छूट इस बात पर निर्भर करता है कि आपके आश्रित की विकलांगता कितनी गंभीर है। अगर आश्रित 40 फीसदी तक विकलांग है तो टैक्स बचत के लिए 75,000 रुपये तक का मेडिकल खर्च कवर किया जा सकता है। अगर आश्रित 80 फीसदी तक विकलांग है तो टैक्स बचत के लिए 1,25,000 रुपये तक का मेडिकल खर्च कवर किया जा सकता है।',\n",
              "   'अगर आप खुद 40 फीसदी से अधिक विकलांग हैं तो आप इस सेक्शन के तहत Income Tax छूट पा सकते हैं। हालांकि, सेक्शन 80यू और सेक्शन 80डीडी का लाभ एक साथ नहीं उठाया जा सकता। इस सेक्शन में भी टैक्स छूट का लाभ सेक्शन 80डीडी की तरह ही होता है। फर्क सिर्फ इतना है कि यह सेक्शन खुद की विकलांगता से जुड़ा है, जबकि सेक्शन 80डी आश्रितों से जुड़ा है।',\n",
              "   'सामाजिक सुरक्षा योजनाओं या निवेश से जुड़े कई विकल्प पर आपको इनकम टैक्स कानून के सेक्शन 80सी के तहत छूट मिलती है। इसमें ईपीएफ, पीपीएफ में निवेश, सुकन्या समृद्धि योजना, एनएससी, टैक्स सेविंग म्यूचुअल फंड (ईएलएसएस) और टैक्स सेविंग एफडी आदि शामिल हैं। \\nइन निवेश विकल्प के माध्यम से की गई बचत पर आप सेक्शन 80सी के तहत इनकम टैक्स छूट का फायदा उठा सकते हैं। इसी तरह जीवन बीमा आदि के प्रीमियम समेत कई अन्य विकल्पों को मिलाकर कुल 1.5 लाख रुपये तक के निवेश पर आप इनकम टैक्स छूट के हकदार होते हैं। \\nइतना ही नहीं दो बच्चों की पढ़ाई में सिर्फ ट्यूशन फीस, होम लोन की किस्त में शामिल मूलधन का हिस्सा, घर की खरीद में स्टांप ड्यूटी और रजिस्ट्रेशन चार्ज आदि पर भी आप सेक्शन 80सी के तहत इनकम टैक्स से छूट का दावा कर सकते हैं।']],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [[{'header': 'सेक्शन 80डी और सेक्शन 80डीडी'},\n",
              "   {'header': 'सेक्शन 80यू'},\n",
              "   {'header': 'सेक्शन 80C'}]],\n",
              " 'distances': [[0.14918804168701172, 0.1544797420501709, 0.15799838304519653]],\n",
              " 'included': [<IncludeEnum.distances: 'distances'>,\n",
              "  <IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the geneartion Model Airavata"
      ],
      "metadata": {
        "id": "7sl_QizLLFQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned, we will be using Airavta, and since it is open-source we will be using transformers and quantization techniques to load the model. You can check more about ways to load open-source LLMs here and here. A T4 GPU environment is needed in collab to run this."
      ],
      "metadata": {
        "id": "ZV_oXFr8L98w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "APdTV2dYcMr9"
      },
      "outputs": [],
      "source": [
        "# !pip install bitsandbytes>=0.39.0\n",
        "# !pip install --upgrade accelerate transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7GGiV4Xb_Yu",
        "outputId": "6ce54c40-eced-4ba7-8e41-c884d68b949b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below functions are used for loading and using Airavata, I used them from it's [official huggingface page](https://huggingface.co/ai4bharat/Airavata\n",
        ")"
      ],
      "metadata": {
        "id": "6A9TYaWjL5pB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "qaihu1g6cVbH"
      },
      "outputs": [],
      "source": [
        "def create_prompt_with_chat_format(messages, bos=\"<s>\", eos=\"</s>\", add_bos=True):\n",
        "    formatted_text = \"\"\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"system\":\n",
        "            formatted_text += \"<|system|>\\n\" + message[\"content\"] + \"\\n\"\n",
        "        elif message[\"role\"] == \"user\":\n",
        "            formatted_text += \"<|user|>\\n\" + message[\"content\"] + \"\\n\"\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            formatted_text += \"<|assistant|>\\n\" + message[\"content\"].strip() + eos + \"\\n\"\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Tulu chat template only supports 'system', 'user' and 'assistant' roles. Invalid role: {}.\".format(\n",
        "                    message[\"role\"]\n",
        "                )\n",
        "            )\n",
        "    formatted_text += \"<|assistant|>\\n\"\n",
        "    formatted_text = bos + formatted_text if add_bos else formatted_text\n",
        "    return formatted_text\n",
        "\n",
        "\n",
        "def inference(input_prompts, model, tokenizer):\n",
        "    input_prompts = [\n",
        "        create_prompt_with_chat_format([{\"role\": \"user\", \"content\": input_prompt}], add_bos=False)\n",
        "        for input_prompt in input_prompts\n",
        "    ]\n",
        "\n",
        "    encodings = tokenizer(input_prompts, padding=True, return_tensors=\"pt\")\n",
        "    encodings = encodings.to(device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(encodings.input_ids, do_sample=False, max_new_tokens=1024)\n",
        "\n",
        "    output_texts = tokenizer.batch_decode(outputs.detach(), skip_special_tokens=True)\n",
        "\n",
        "    input_prompts = [\n",
        "        tokenizer.decode(tokenizer.encode(input_prompt), skip_special_tokens=True) for input_prompt in input_prompts\n",
        "    ]\n",
        "    output_texts = [output_text[len(input_prompt) :] for input_prompt, output_text in zip(input_prompts, output_texts)]\n",
        "    return output_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "ba1f944bc662433db2860ead426051f2",
            "99d9a968e83549fc928c70a18e77194e",
            "4104c947f5794fe9a30e42444959c4f9",
            "3e47f149626a4fe4885dee00f8b1b110",
            "7af8f8cb76c14613aa9f41f0fcdc53b8",
            "af95a200b4644d06a86f40b40c0a37a9",
            "6dca6e3522124f968ef5d6446ae5d49d",
            "b26ea9406b6b4ac58db7e3313b82e08f",
            "b69a79134ecb454daad8219c0a0fd914",
            "30dfc4cd830c46e7b7354d96b7bc5e1e",
            "aa8bdf1c538e4ed7b07b6539b2c2d98b"
          ]
        },
        "id": "EZdcWTBQctL8",
        "outputId": "1945577d-20db-44b5-cbde-d4e4d69ae32b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba1f944bc662433db2860ead426051f2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_name = \"ai4bharat/Airavata\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,  quantization_config=quantization_config, torch_dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the interesting part: prompt to generate the answer. Here, we create a prompt that instructs the language model to generate answers based on specific guidelines. The instructions are simple: first, the model reads and understands the question, then reviews the context provided. It uses this information to craft a clear, concise, and accurate response. If you look at it carefully, this is the Hindi version of the typical RAG prompt."
      ],
      "metadata": {
        "id": "NOOxmftlMNzT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "uYS025cffcZS"
      },
      "outputs": [],
      "source": [
        "prompt ='''आप एक बड़े भाषा मॉडल हैं जो दिए गए संदर्भ के आधार पर सवालों का उत्तर देते हैं। नीचे दिए गए निर्देशों का पालन करें:\n",
        "\n",
        "1. **प्रश्न पढ़ें**:\n",
        "    - दिए गए सवाल को ध्यान से पढ़ें और समझें।\n",
        "\n",
        "2. **संदर्भ पढ़ें**:\n",
        "    - नीचे दिए गए संदर्भ को ध्यानपूर्वक पढ़ें और समझें।\n",
        "\n",
        "3. **सूचना उत्पन्न करना**:\n",
        "    - संदर्भ का उपयोग करते हुए, प्रश्न का विस्तृत और स्पष्ट उत्तर तैयार करें।\n",
        "    - यह सुनिश्चित करें कि उत्तर सीधा, समझने में आसान और तथ्यों पर आधारित हो।\n",
        "\n",
        "### उदाहरण:\n",
        "\n",
        "**संदर्भ**:\n",
        "    \"नई दिल्ली भारत की राजधानी है और यह देश का प्रमुख राजनीतिक और प्रशासनिक केंद्र है। यह शहर ऐतिहासिक स्मारकों, संग्रहालयों और विविध संस्कृति के लिए जाना जाता है।\"\n",
        "\n",
        "**प्रश्न**:\n",
        "    \"भारत की राजधानी क्या है और यह क्यों महत्वपूर्ण है?\"\n",
        "\n",
        "**प्रत्याशित उत्तर**:\n",
        "    \"भारत की राजधानी नई दिल्ली है। यह देश का प्रमुख राजनीतिक और प्रशासनिक केंद्र है और ऐतिहासिक स्मारकों, संग्रहालयों और विविध संस्कृति के लिए जाना जाता है।\"\n",
        "\n",
        "### निर्देश:\n",
        "\n",
        "अब, दिए गए संदर्भ और प्रश्न का उपयोग करके उत्तर दें:\n",
        "\n",
        "**संदर्भ**:\n",
        "{docs}\n",
        "\n",
        "**प्रश्न**:\n",
        "{query}\n",
        "\n",
        "उत्तर:\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting it all together"
      ],
      "metadata": {
        "id": "2r8xsQKnMTXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query):\n",
        "  docs =  collection.query(\n",
        "    query_texts=[query],\n",
        "    n_results=3\n",
        ")\n",
        "  docs = [doc for doc in docs['documents'][0]]\n",
        "  docs = \"\\n\".join(docs)\n",
        "  formatted_prompt = prompt.format(docs = docs,query = query)\n",
        "  answers = inference([formatted_prompt], model, tokenizer)\n",
        "  return answers[0]"
      ],
      "metadata": {
        "id": "RGyQdYLIMVyO"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF1BNw63gR9r",
        "outputId": "a4a29032-43e0-4901-c49a-b6e66ec2fea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: सेक्शन 80डीडी के तहत विकलांग आश्रित के लिए कौन से मेडिकल खर्च पर टैक्स छूट मिल सकती है?\n",
            "Answer: आश्रित व्यक्ति के लिए टैक्स छूट के लिए पात्र होने के लिए, उन्हें 40% से अधिक विकलांग होना चाहिए। यदि वे 40% से अधिक विकलांग हैं, तो वे 40,000 रुपये तक के चिकित्सा खर्च पर कर छूट प्राप्त कर सकते हैं। यदि वे 60% से अधिक विकलांग हैं, तो वे 60,000 रुपये तक के चिकित्सा खर्च पर कर छूट प्राप्त कर सकते हैं। यदि वे 80% से अधिक विकलांग हैं, तो वे 80,000 रुपये तक के चिकित्सा खर्च पर कर छूट प्राप्त कर सकते हैं।\n",
            "\n",
            "Question: क्या सेक्शन 80यू और सेक्शन 80डीडी का लाभ एक साथ उठाया जा सकता है?\n",
            "Answer: नहीं।\n",
            "\n",
            "Question: सेक्शन 80 C की लिमिट क्या होती है?\n",
            "Answer: सेक्शन 80 सी की सीमा 1.5 लाख रुपये है।\n",
            "\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    'सेक्शन 80डीडी के तहत विकलांग आश्रित के लिए कौन से मेडिकल खर्च पर टैक्स छूट मिल सकती है?',\n",
        "    'क्या सेक्शन 80यू और सेक्शन 80डीडी का लाभ एक साथ उठाया जा सकता है?',\n",
        "    'सेक्शन 80 C की लिमिट क्या होती है?'\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    answer = generate_answer(question)\n",
        "    print(f\"Question: {question}\\nAnswer: {answer}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5fxNr01RMbbg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ba1f944bc662433db2860ead426051f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99d9a968e83549fc928c70a18e77194e",
              "IPY_MODEL_4104c947f5794fe9a30e42444959c4f9",
              "IPY_MODEL_3e47f149626a4fe4885dee00f8b1b110"
            ],
            "layout": "IPY_MODEL_7af8f8cb76c14613aa9f41f0fcdc53b8"
          }
        },
        "99d9a968e83549fc928c70a18e77194e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af95a200b4644d06a86f40b40c0a37a9",
            "placeholder": "​",
            "style": "IPY_MODEL_6dca6e3522124f968ef5d6446ae5d49d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4104c947f5794fe9a30e42444959c4f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b26ea9406b6b4ac58db7e3313b82e08f",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b69a79134ecb454daad8219c0a0fd914",
            "value": 3
          }
        },
        "3e47f149626a4fe4885dee00f8b1b110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30dfc4cd830c46e7b7354d96b7bc5e1e",
            "placeholder": "​",
            "style": "IPY_MODEL_aa8bdf1c538e4ed7b07b6539b2c2d98b",
            "value": " 3/3 [01:26&lt;00:00, 27.86s/it]"
          }
        },
        "7af8f8cb76c14613aa9f41f0fcdc53b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af95a200b4644d06a86f40b40c0a37a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dca6e3522124f968ef5d6446ae5d49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b26ea9406b6b4ac58db7e3313b82e08f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b69a79134ecb454daad8219c0a0fd914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30dfc4cd830c46e7b7354d96b7bc5e1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa8bdf1c538e4ed7b07b6539b2c2d98b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}